---
title: Using Pre-Trained LLMs
jupyter: 
  kernelspec:
    name: "conda-env-llm-env-py"
    language: "python"
    display_name: "llm-env"
---

The 2 approaches in langchain are to use `pipeline` and `AutoModel` approaches.

`pipeline` abstracts more away than `AutoModel`. `AutoModel` gives you more
control over tokenization and model selection and is suitable for a broader
range of language tasks.

## Using `AutoModel` to Classify

```{python}
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering
import torch
from datasets import load_dataset

```

```{python}
model_nm = "textattack/distilbert-base-uncased-SST-2"

# Load the tokenizer and pre-trained model
tokenizer = AutoTokenizer.from_pretrained(model_nm)
model = AutoModelForSequenceClassification.from_pretrained(
  model_nm, num_labels=2)

text = ["A masterpiece movie. Essential viewing.", "90 minutes of my life I'll never get back."]

# Tokenize inputs and pass them to the model for inference
inputs = tokenizer(text, return_tensors="pt", padding=True)
# type is  transformers.tokenization_utils_base.BatchEncoding
# inputs["input_ids"]
# tensor([[  101,  1037, 17743,  3185,  1012,  6827, 10523,  1012,   102,     0,
#              0,     0,     0,     0],
#         [  101,  3938,  2781,  1997,  2026,  2166,  1045,  1005,  2222,  2196,
#           2131,  2067,  1012,   102]])
# inputs["attention_mask"]
# tensor([[  101,  1037, 17743,  3185,  1012,  6827, 10523,  1012,   102,     0,
#              0,     0,     0,     0],
#         [  101,  3938,  2781,  1997,  2026,  2166,  1045,  1005,  2222,  2196,
#           2131,  2067,  1012,   102]])

outputs = model(**inputs)
# SequenceClassifierOutput(loss=None, logits=tensor([[-0.8427,  0.7898],
#         [ 0.4062, -0.4545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
logits = outputs.logits
# tensor([[-0.8427,  0.7898],
#         [ 0.4062, -0.4545]], grad_fn=<AddmmBackward0>)
```

```{python}
pred_classes = torch.argmax(logits, dim=1).tolist()
# [1, 0]
for idx, class_ in enumerate(pred_classes):
    print(f"Predicted class for \"{text[idx]}\": {class_}")
```

## Using `AutoModel` for Sequence Generation

```{python}
prompt = """
Say your prayers, little one
Don't forget, my son
To include everyone
I tuck you in, warm within
Keep you free from sin
'Til the Sandman, he comes

"""

```

```{python}
model_nm = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_nm)
model = AutoModelForCausalLM.from_pretrained(model_nm)
inputs = tokenizer.encode(prompt, return_tensors="pt")
output = model.generate(inputs, max_length=100)
out_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(out_text[len(prompt):])

# I'll take you to the moon
# And I'll take you to the stars
# And I'll take you to the stars
# And I'll take you to the stars
# And I'll...

# wow...

```

## Summarisation

Training relies on a corpus of documents that have already been summarised (the labels).

### Extractive Summarisation

* Uses encoder models (BERT) or encoder:decoder models (T5)
* Detects the portions of the text responsible for meaning and uses them in an abridged summary.

### Abstractive Summarisation

* Uses sequence to sequence LLMs
* Creates output based on input prompt word by word, may use completely different language.


```{python}
articles = load_dataset("ILSUM/ILSUM-1.0", "English")
articles = articles["train"]
```


```{python}
articles[0]["Article"]

```

```{python}
articles[0]["Summary"]

```

```{python}
model_nm = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_nm)
model = AutoModelForSeq2SeqLM.from_pretrained(model_nm)
# at this point, we should add our prompt to the tokenizer's encode method
# return_tensors="pt" ensures we get pytorch tensors back
input_ids = tokenizer.encode("summarise the following text: " + articles[0]["Article"], return_tensors="pt", max_length=500, truncation=True)
# lets generate the summary
summary_ids = model.generate(input_ids, max_length=100)
output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
output

```



```{python}
opinosis = load_dataset("opinosis")
model_nm = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_nm)
model = AutoModelForSeq2SeqLM.from_pretrained(model_nm)
example = opinosis['train'][-1]['review_sents']
input_ids = tokenizer.encode("summarize: " + example, return_tensors="pt", max_length=512, truncation=True)
summary_ids = model.generate(input_ids, max_length=150)
summary = tokenizer.decode(
  summary_ids[0], skip_special_tokens=True)
summary
# This is pretty good, though be careful as the text in the last index of this dataset contains several reviews of a Garmin satnav with differing opinions...

```

## Translation

Trained with a corpus of articles in the source language and labels in the target language. Encoder:Decoder models are required for this job.


```{python}
model_name = "Helsinki-NLP/opus-mt-en-es"

# Load the tokenizer and model checkpoint
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

english_inputs = ["What would you like to know?", "My mother", "Where am I?", "Which way to the beach?", "See you next week!"]

# Encode inputs, generate translations, decode, and print
for english_input in english_inputs:
    input_ids = tokenizer.encode(english_input, return_tensors="pt")
    translated_ids = model.generate(input_ids)
    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)
    print(f"English: {english_input} | Spanish: {translated_text}")

```


## Question Answering

3 main forms:

1. Extractive
2. Open Generative
3. Closed Generative

| Method            | Architecture    | Approach |
| ----------------- | --------------- | -------- |
| Extractive        | Encoder         | Finds the answer directly in the text |
| Open Generative   | Encoder:Decoder | Produces its own answer but based\nupon a document context |
| Closed Generative | Decoder         | The answer is generated from the\nmodels own comprehension only |




```{python}
qa_data = load_dataset("xtreme", name="MLQA.en.en")


```

```{python}
qa_data["test"][0]
# 'question': 'Who analyzed the biopsies?',
# 'answers': {'answer_start': [457],
#   'text': ['Rutgers University biochemists']
qa_data["test"][0]["context"][457:500]
# Task is therefore a supervised classification
```

```{python}
mod_checkpoint = "deepset/minilm-uncased-squad2"
# this is already fine-tuned on the squad dataset
tokenizer = AutoTokenizer.from_pretrained(mod_checkpoint)
Q = "Which organisations were being sued?"
inputs = tokenizer(Q, qa_data["test"][0]["context"], return_tensors="pt")

```

Checking `inputs.keys()` yields:

dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])

'input_ids': tensor([[  101,  2040,  2020,  2108, 12923,  1029,   102,  1999,  2807,  1010,...

'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,

```{python}
model = AutoModelForQuestionAnswering.from_pretrained(mod_checkpoint)

with torch.no_grad():
  outputs = model(**inputs)
# odict_keys(['start_logits', 'end_logits'])

```

```{python}
start_id = torch.argmax(outputs.start_logits) # find max likelihood for start ID
end_id = torch.argmax(outputs.end_logits) + 1 # ensure last token is included
print(f"Answer indices are {start_id}:{end_id}")

```

```{python}
# return answer by chopping up the input IDs with the produced indexes
answer_ids = inputs["input_ids"][0][start_id:end_id]
response = tokenizer.decode(answer_ids)
response
# Produces the wrong answer!!
# 'five unnamed civilian contractors and the widows of contractors walter kasza and robert frost'
# These are the complainants
```


### Context Window

If the context is longer than the context window, we can adapt the tokenizer to chunk the context.

```{python}
# Find the largest context in the data
max_id = ""
max_len = 0
for i in qa_data["test"]:
  if len(i["context"]) > max_len:
    max_id = i["id"]
    max_len = len(i["context"])

print(f'{max_id}: {max_len} words')

```

```{python}
for i in qa_data["test"]:
  if max_id == i["id"]:
    longest_example = i
longest_example
# It's about Kiss (the band)
#  'question': 'What final name was settled on as a stage name?',
#  'answers': {'answer_start': [8253], 'text': ['Vinnie Vincent']}}
```

To chunk the context window up, we use `return_overflowing_tokens` with a max length and stride.

```{python}
long_tokens = tokenizer(longest_example["question"], longest_example["context"], return_overflowing_tokens=True, max_length=100, stride=25)

for i, window in enumerate(long_tokens["input_ids"]):
  print(f"Window {i} has {len(window)} tokens")
# Window 35 has 100 tokens
# Window 36 has 100 tokens
# Window 37 has 85 tokens
```

Final window will tend to be shorter unless the length of the context window modulo max_length == 0. For further context you can decode each window.

```{python}
for window in long_tokens["input_ids"]:
  print(tokenizer.decode(window).split("[SEP]")[1])
  
# roving window with following pattern
# [CLS] <QUESTION> [SEP] <ROVING_CONTEXT_WINDOW> [SEP]

```
