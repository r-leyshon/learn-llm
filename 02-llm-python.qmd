---
title: LLMs in Python
jupyter: 
  kernelspec:
    name: "conda-env-llm-env-py"
    language: "python"
    display_name: "llm-env"
---

## Orientation

Largely with Hugging Face's `transformers` package.

```{python}
#| warning: false
import requests
from bs4 import BeautifulSoup
from transformers import pipeline # relies on torch
import torch

```

### Text Classification

![Toranaga San on tellyvisions](https://tellyvisions.org/sites/default/files/styles/hero__1070x485/public/2023-12/Shogun%20Hiroyuki%20Sanada.jpeg?h=722d8239&itok=4P2HrbGh)

I've recently been watching Shōgun on Disney+. Here are some Google reviews to
attempt to classify, I included a 5 star and 1 star review respectively.

```{python}
#| warning: false
reviews = [""""Shogun" is a captivating TV series that offers a refreshing and
unique storyline, making it a standout in today's television landscape. From
its inception, it immerses viewers in a world that is both historically rich
and artistically mesmerizing.

One of the most compelling aspects of "Shogun" is its distinctiveness. In an
era where many shows follow predictable patterns, "Shogun" breaks the mold with
its innovative storytelling. It presents a narrative that is both engaging and
unpredictable, keeping audiences on the edge of their seats throughout each
episode.

The cinematography and costume design in "Shogun" are nothing short of
stunning. Every frame is a work of art, meticulously crafted to transport
viewers to feudal Japan. The attention to detail in the costumes not only adds
authenticity but also enhances the overall visual experience of the series.

Beyond its aesthetic appeal, "Shogun" offers a glimpse into ancient Japanese
culture, providing viewers with a rich tapestry of traditions, customs, and
values. This cultural exploration is not only educational but also serves as a
gateway to a deeper understanding and appreciation of history.

Personally, "Shogun" has been a delightful journey that not only entertained
but also enlightened me about a fascinating period in Japanese history. It's a
testament to the power of storytelling when combined with exceptional
cinematography and cultural immersion.

In conclusion, "Shogun" is a must-watch for anyone seeking a captivating,
visually stunning, and culturally enriching television experience. It's a
breath of fresh air in an industry often inundated with familiar tropes,
offering something truly unique and memorable.

""",
"""
Having read everything James Clavell ever wrote including Shogun at least five
times, and also being a huge fan of the 1980 mini-series starring Richard
Chamberlain, Toshiro Mifune and Yoko Shimada, I eagerly awaited the airing of
the new FX/Hulu adaptation of the novel. I confess to be being disappointed. I
certainly didn’t expect the next version to be a mere remake on the 44 year-old
classic, but I’d have thought that nearly half a century would have provided
ample time for better costumes, better sets, better casting and a screenplay
that takes fewer liberties in the telling of what is already a wonderful story.
The characterizations in the remake are flat and largely boring. The sole cast
member I feel does justice to the their role is Hiroyuki Sanada as Lord
Toranaga.  Lastly, the love affair between Anjinsan and Mariko has zero heat
and passion and is wholly unbelievable.  Maybe we’ll need to wait 44 years for
someone to give this novel a new treatment worth watching again and again. This
new one fails at that.
""",
]

classy = pipeline(
    "text-classification",
    model="nlptown/bert-base-multilingual-uncased-sentiment")
preds = [classy(review) for review in reviews]
print(preds)

```

### Text Summarisation

The t5 models are indicated to be good for this. Using the
[hugging face model directory](https://huggingface.co/models) find a good match for the job you need.

Let's get a summary of a page from my website:

<iframe src="https://thedatasavvycorner.com/music-reviews/04-triads" width=800 height=450></iframe>

```{python}
#| warning: false
# scrape the text
resp = requests.get("https://thedatasavvycorner.com/music-reviews/04-triads")
txt = resp.text
soup = BeautifulSoup(txt, "html.parser")
# Use a summarization model
llm = pipeline(
    "summarization", model="pszemraj/long-t5-tglobal-base-16384-book-summary")
outputs = llm(soup.text, max_length=100)
print(outputs[0]["summary_text"])
```

### Question-Answering

```{python}
#| warning: false
llm = pipeline(
    "question-answering",
    # model="deepset/roberta-base-squad2"
    )
q = """What is the genre of this album?"""
response = llm(context=soup.text, question=q)
print(response["answer"])

```

### Translation

```{python}
anthem = """
Mae hen wlad fy nhadau yn annwyl i mi
Gwlad beirdd a chantorion, enwogion o fri
Ei gwrol ryfelwyr, gwladgarwyr tra mâd
Tros ryddid gollasant eu gwaed
Gwlad, Gwlad, pleidiol wyf i'm gwlad
Tra môr yn fur i'r bur hoff bau
O bydded i'r heniaith barhau
"""

translator = pipeline(task="translation_CY_to_EN", model="Helsinki-NLP/opus-mt-cy-en")
inenglish = translator(anthem)
print(inenglish[0]["translation_text"])

```

Not quite right haha!

## Transformers

{{< video https://vimeo.com/947964306 >}}


* No RNNs
* Attention & positional encoding
* Simultaneous processing of tokens

![transformer architecture on Wikipedia](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)

The encoder is the architecture to the left and the decoder is to the right of the image. Note that not all models use an encoder and decoder, dependant upon the tasks they are designed to perform. 

| Model           | Task                          | Examples | 
| --------------- | ----------------------------- | -------- |
| Encoder         | Classification, Extractive QA | BERT     |
| Decoder         | Text gen, Gen QA              | GPT      |
| Encoder-Decoder | Translation, Summarisation    | BART, T5 |

We can access transformer architecture through the **pytorch** package. The `torch.nn.Transformer` class takes the following parameters:

* `d_model` = Number of dimensions in the model.
* `nhead` = Number of attention heads. Usually a divisor of `d_model`.
* `num_encoder_layers` = Depth of encoder.
* `num_decoder_layers` = Depth of decoder.

## Build a Transformer

Using Pytorch.

```{python}
optimus = torch.nn.Transformer(
    d_model=50,
    nhead=5,
    num_encoder_layers=3,
    num_decoder_layers=3
)
print(optimus)
```
