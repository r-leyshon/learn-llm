---
title: LLMs in Python
jupyter: 
  kernelspec:
    name: "conda-env-llm-env-py"
    language: "python"
    display_name: "llm-env"
---

## Orientation

Largely with Hugging Face's `transformers` package.

```{python}
#| warning: false
import math

import requests
from bs4 import BeautifulSoup
from transformers import pipeline # relies on torch
import torch

```

### Text Classification

![Toranaga San on tellyvisions](https://tellyvisions.org/sites/default/files/styles/hero__1070x485/public/2023-12/Shogun%20Hiroyuki%20Sanada.jpeg?h=722d8239&itok=4P2HrbGh)

I've recently been watching Shōgun on Disney+. Here are some Google reviews to
attempt to classify, I included a 5 star and 1 star review respectively.

```{python}
#| warning: false
reviews = [""""Shogun" is a captivating TV series that offers a refreshing and
unique storyline, making it a standout in today's television landscape. From
its inception, it immerses viewers in a world that is both historically rich
and artistically mesmerizing.

One of the most compelling aspects of "Shogun" is its distinctiveness. In an
era where many shows follow predictable patterns, "Shogun" breaks the mold with
its innovative storytelling. It presents a narrative that is both engaging and
unpredictable, keeping audiences on the edge of their seats throughout each
episode.

The cinematography and costume design in "Shogun" are nothing short of
stunning. Every frame is a work of art, meticulously crafted to transport
viewers to feudal Japan. The attention to detail in the costumes not only adds
authenticity but also enhances the overall visual experience of the series.

Beyond its aesthetic appeal, "Shogun" offers a glimpse into ancient Japanese
culture, providing viewers with a rich tapestry of traditions, customs, and
values. This cultural exploration is not only educational but also serves as a
gateway to a deeper understanding and appreciation of history.

Personally, "Shogun" has been a delightful journey that not only entertained
but also enlightened me about a fascinating period in Japanese history. It's a
testament to the power of storytelling when combined with exceptional
cinematography and cultural immersion.

In conclusion, "Shogun" is a must-watch for anyone seeking a captivating,
visually stunning, and culturally enriching television experience. It's a
breath of fresh air in an industry often inundated with familiar tropes,
offering something truly unique and memorable.

""",
"""
Having read everything James Clavell ever wrote including Shogun at least five
times, and also being a huge fan of the 1980 mini-series starring Richard
Chamberlain, Toshiro Mifune and Yoko Shimada, I eagerly awaited the airing of
the new FX/Hulu adaptation of the novel. I confess to be being disappointed. I
certainly didn’t expect the next version to be a mere remake on the 44 year-old
classic, but I’d have thought that nearly half a century would have provided
ample time for better costumes, better sets, better casting and a screenplay
that takes fewer liberties in the telling of what is already a wonderful story.
The characterizations in the remake are flat and largely boring. The sole cast
member I feel does justice to the their role is Hiroyuki Sanada as Lord
Toranaga.  Lastly, the love affair between Anjinsan and Mariko has zero heat
and passion and is wholly unbelievable.  Maybe we’ll need to wait 44 years for
someone to give this novel a new treatment worth watching again and again. This
new one fails at that.
""",
]

classy = pipeline(
    "text-classification",
    model="nlptown/bert-base-multilingual-uncased-sentiment")
preds = [classy(review) for review in reviews]
print(preds)

```

### Text Summarisation

The t5 models are indicated to be good for this. Using the
[hugging face model directory](https://huggingface.co/models) find a good match
for the job you need.

Let's get a summary of a page from my website:

<iframe src="https://thedatasavvycorner.com/music-reviews/04-triads" width=800 height=450></iframe>

```{python}
#| warning: false
# scrape the text
resp = requests.get("https://thedatasavvycorner.com/music-reviews/04-triads")
txt = resp.text
soup = BeautifulSoup(txt, "html.parser")
# Use a summarization model
llm = pipeline(
    "summarization", model="pszemraj/long-t5-tglobal-base-16384-book-summary")
outputs = llm(soup.text, max_length=100)
print(outputs[0]["summary_text"])
```

### Question-Answering

```{python}
#| warning: false
llm = pipeline(
    "question-answering",
    # model="deepset/roberta-base-squad2"
    )
q = """What is the genre of this album?"""
response = llm(context=soup.text, question=q)
print(response["answer"])

```

### Translation

```{python}
anthem = """
Mae hen wlad fy nhadau yn annwyl i mi
Gwlad beirdd a chantorion, enwogion o fri
Ei gwrol ryfelwyr, gwladgarwyr tra mâd
Tros ryddid gollasant eu gwaed
Gwlad, Gwlad, pleidiol wyf i'm gwlad
Tra môr yn fur i'r bur hoff bau
O bydded i'r heniaith barhau
"""

translator = pipeline(task="translation_CY_to_EN", model="Helsinki-NLP/opus-mt-cy-en")
inenglish = translator(anthem)
print(inenglish[0]["translation_text"])

```

Not quite right haha!

## Transformers

{{< video https://vimeo.com/947964306 >}}


* No RNNs
* Attention & positional encoding
* Simultaneous processing of tokens

![transformer architecture on Wikipedia](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)

The encoder is the architecture to the left and the decoder is to the right of
the image. Note that not all models use an encoder and decoder, dependant upon
the tasks they are designed to perform. 

| Model           | Task                          | Examples | 
| --------------- | ----------------------------- | -------- |
| Encoder         | Classification, Extractive QA | BERT     |
| Decoder         | Text gen, Gen QA              | GPT      |
| Encoder-Decoder | Translation, Summarisation    | BART, T5 |

We can access transformer architecture through the **pytorch** package. The
`torch.nn.Transformer` class takes the following parameters:

* `d_model` = Number of dimensions in the model.
* `nhead` = Number of attention heads. Usually a divisor of `d_model`.
* `num_encoder_layers` = Depth of encoder.
* `num_decoder_layers` = Depth of decoder.

## Build a Transformer

Using Pytorch.

```{python}
optimus = torch.nn.Transformer(
    d_model=50,
    nhead=5,
    num_encoder_layers=3,
    num_decoder_layers=3
)
print(optimus)
```

## Add Positional Encoding

This makes models more effective. As transformers process tokens
simultaneously, adding positional encoding adds sequence order into the models.

Positional encoding gets added to embeddings, stored in the form of pytorch
tensors. Without this positional information, self-attention mechanisms will
not work properly.

For more on positional encoding, see [this tutorial](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/).

![Positional encoding table](https://machinelearningmastery.com/wp-content/uploads/2022/01/PE3.png)

Here, we subclass from the torch.nn module class like so:

```{python}
# Subclass an appropriate PyTorch class 
class PositionalEncoder(torch.nn.Module):
    def __init__(self, dim_mod: int, max_len: int) -> None:
        super(PositionalEncoder, self).__init__()
        self.dim_mod = dim_mod
        self.max_len = max_len
        
        # Initialize positional encoding matrix for token positions in
        # sequences up max_len
        pe = torch.zeros(max_len, dim_mod)

        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(
                0, dim_mod, 2, dtype=torch.float
                ) * -(math.log(10000.0) / dim_mod)
            )
        
        # Assign unique position encodings to matrix by alternating sin & cos
        pe[:, 0::2] = torch.sin(pos * div_term)
        pe[:, 1::2] = torch.cos(pos * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    # Update an embeddings tensor with positional encodings
    def forward(self, x:torch.tensor) -> torch.tensor:
        x = x + self.pe[:, :x.size(1)]
        return x

```


```{python}
some_tensor = torch.tensor([range(0, 10)])
# v.brittle, if dim_mod does not match length of tensor, RuntimeError. Refactor
# to extract len from tensor...
pos_encoder = PositionalEncoder(10, 2)
pos_encoder.forward(some_tensor)

```

## Attention

```{python}

class MultiHeadAttention(torch.nn.Module):
    def __init__(self, dim_mod: int, n_heads:int) -> None:
        super(MultiHeadAttention, self).__init__()
        # Set the num of attention heads
        self.n_heads = n_heads
        self.dim_mod = dim_mod
        self.head_dim = dim_mod // n_heads
		# Set up the linear transformations
        self.query_linear = torch.nn.Linear(dim_mod, dim_mod)
        self.key_linear = torch.nn.Linear(dim_mod, dim_mod)
        self.value_linear = torch.nn.Linear(dim_mod, dim_mod)
        self.output_linear = torch.nn.Linear(dim_mod, dim_mod) # this will concat
        # query, key and value 


    def split_heads(self, x:torch.tensor, batch_size:int) -> torch.tensor:
        # Split sequence embeddings across attention heads
        x = x.view(batch_size, -1, self.n_heads, self.head_dim)
        return x.permute(0, 2, 1, 3).contiguous().view(
            batch_size * self.n_heads, -1, self.head_dim)


    def compute_attention(self, query, key, mask=None):
        # Compute dot-product (cosine similarity) attention scores
        scores = torch.matmul(query, key.permute(1, 2, 0))
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float("-1e20"))
        # Normalize attention scores into attention weights
        attention_weights = torch.nn.functional.softmax(scores, dim=-1)
        return attention_weights


    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        query = self.split_heads(self.query_linear(query), batch_size)
        key = self.split_heads(self.key_linear(key), batch_size)
        value = self.split_heads(self.value_linear(value), batch_size)

        attention_weights = self.compute_attention(query, key, mask)
            
        # Multiply attention weights by values, concatenate and linearly
        # project outputs
        output = torch.matmul(attention_weights, value)
        output = output.view(
            batch_size, self.n_heads, -1, self.head_dim
            ).permute(0, 2, 1, 3).contiguous().view(
                batch_size, -1, self.dim_mod)
        return self.output_linear(output)


```

## Build an Encoder

```{python}
class FeedForwardSubLayer(torch.nn.Module):
    # Specify the two linear layers' input and output sizes
    def __init__(self, dim_mod, d_ff):
        super(FeedForwardSubLayer, self).__init__()
        self.fc1 = torch.nn.Linear(dim_mod, d_ff)
        self.fc2 = torch.nn.Linear(d_ff, dim_mod)
        self.relu = torch.nn.ReLU()

	# Apply a forward pass
    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

```


```{python}
class EncoderLayer(torch.nn.Module):
    def __init__(self, dim_mod, n_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(dim_mod, n_heads)
        self.feed_forward = FeedForwardSubLayer(dim_mod, d_ff)
        self.norm1 = torch.nn.LayerNorm(dim_mod)
        self.norm2 = torch.nn.LayerNorm(dim_mod)
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x, mask):
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        ff_output = self.feed_forward(x)
        return self.norm2(x + self.dropout(ff_output))

```

```{python}
class TransformerEncoder(torch.nn.Module):
    def __init__(self, vocab_size, dim_mod, num_layers, n_heads, d_ff, dropout, max_sequence_length):
        super(TransformerEncoder, self).__init__()
        self.embedding = torch.nn.Embedding(vocab_size, dim_mod)
        self.positional_encoding = PositionalEncoder(dim_mod, max_sequence_length)
        # Define a stack of multiple encoder layers
        self.layers = torch.nn.ModuleList([EncoderLayer(dim_mod, n_heads, d_ff, dropout) for _ in range(num_layers)])
	
    # Complete the forward pass method
    def forward(self, x, mask):
        x = self.embedding(x)
        x = self.positional_encoding(x)
        for layer in self.layers:
            x = layer(x, mask)
        return x

class ClassifierHead(torch.nn.Module):
    def __init__(self, dim_mod, num_classes):
        super(ClassifierHead, self).__init__()
        # Add linear layer for multiple-class classification
        self.fc = torch.nn.Linear(dim_mod, num_classes)

    def forward(self, x):
        logits = self.fc(x[:, 0, :])
        # Obtain log class probabilities upon raw outputs
        return torch.nn.functional.log_softmax(logits, dim=-1)

```

### Test the Encoder Transformer

This uses random masks, though in practice this mask should correspond to
padding token locations originating from the text you are processing.

```{python}
num_classes = 3
vocab_size = 10000
batch_size = 8
d_model = 512
num_heads = 8
num_layers = 6
d_ff = 2048
sequence_length = 64 # needs to be same size as tensor
dropout = 0.1

```


```{python}
input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))
mask = torch.randint(0, 2, (sequence_length, sequence_length))

# Instantiate the encoder transformer's body and head
encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)
classifier = ClassifierHead(d_model, num_classes)

# Complete the forward pass 
output = encoder.forward(input_sequence, mask)
classification = classifier(output)
print("Classification outputs for a batch of ", batch_size, "sequences:")
print(classification)

```

## Build a Decoder

In autoregressive LLMs like chatGPT, only a decoder Is used. The difference is
the mask that is used, here it is referred to as a triangular mask, with a
split of 1s and 0s on the diaganol. Each new token can only pay attention to
previous tokens, hence 'predict the next word'.

```{python}
class TransformerDecoder(torch.nn.Module):
    def __init__(
        self,
        vocab_size,
        d_model,
        num_layers,
        num_heads,
        d_ff,
        dropout,
        max_sequence_length):
        super(TransformerDecoder, self).__init__()
        self.embedding = torch.nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoder(
            d_model, max_sequence_length)
        self.layers = torch.nn.ModuleList(
            [EncoderLayer(
                d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

        # Add a linear layer (head) for next-word prediction
        self.fc = torch.nn.Linear(d_model, vocab_size)

    def forward(self, x, self_mask):
        x = self.embedding(x)
        x = self.positional_encoding(x)
        for layer in self.layers:
            x = layer(x, self_mask)

        # Apply the forward pass through the model head
        x = self.fc(x)
        return torch.nn.functional.log_softmax(x, dim=-1)
```

### Test the decoder

```{python}
num_classes = 3
vocab_size = 10000
batch_size = 8
d_model = 512
num_heads = 8
num_layers = 6
d_ff = 2048
sequence_length = 64 # RunTimeError, must match tensor sizes
dropout = 0.1
input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))
# Create a triangular attention mask for causal attention
self_attention_mask = (1 - torch.triu(
    torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()
print(self_attention_mask)

```

```{python}
# Instantiate the decoder transformer
decoder = TransformerDecoder(
    vocab_size, d_model, num_layers, num_heads, d_ff, dropout,
    max_sequence_length=sequence_length)
output = decoder(input_sequence, self_attention_mask)
print(output.shape)
print(output)

```
